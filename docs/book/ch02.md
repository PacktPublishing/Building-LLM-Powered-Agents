# Building Your First Conversational Agent

Building a basic AI agent isn't as complex as you might imagine. With the right tools and approach, you can create a basic conversational agent in just a few lines of code. However, a simple agent has limited utility. The real power emerges as you progressively enhance its capabilities, making it smarter and more versatile.

In this chapter, we'll introduce Winston, our cognitive AI agent project that we'll develop throughout this book. We'll start with Winston's most basic form - a conversational interface to an LLM - and gradually expand its abilities. This approach allows us to explore the fundamental concepts of AI agent development while building a practical, evolving system.

We'll begin by creating Winston's core conversational functionality. Then, we'll add streaming responses for more natural interactions, implement memory so Winston can maintain context, and introduce tool usage for performing simple tasks like checking the weather.

By the end of this chapter, you'll have laid the foundation for Winston - an AI agent capable of engaging in conversation and executing basic tasks. More importantly, you'll gain insight into the underlying mechanisms of these systems. This knowledge will serve as the cornerstone for developing more advanced cognitive capabilities in Winston as we progress through the book.

## Chapter Overview

In this chapter we're going to cover the following main topics:

- Creating Winston's basic conversational interface using Chainlit and LiteLLM
- Implementing streaming responses and conversation history
- Defining Winston's initial persona through a system prompt
- Integrating tool usage to extend Winston's capabilities
- Managing function calls while maintaining streaming functionality

## Introducing Winston

Throughout this book, we'll build a cognitive AI agent called Winston, a complex system that can think, plan, and learn. We'll construct Winston piece by piece mirroring how real AI systems are developed.

Winston will showcase key abilities of advanced AI: working independently, pursuing goals, thinking through problems, making plans, understanding its environment, interacting with humans, and improving over time.

We'll develop Winston in the following stages throughout this book:

1. **Basic conversational skills**: In this chapter, we'll start with basic conversation skills, using a language model to let Winston communicate.
2. **Knowledge and memory**: In chapters 4 and 5, we'll give Winston ways to dynamically store and retrieve information using vector indexing and a knowledge graph.
3. **Tool user, planning and reasoning**: In chapters 6 and 7, we’ll endow Winston with the ability to break down tasks and interact with external systems to carry out actions.
4. **Learning from experience**: As Winston becomes more autonomous and capable, in chapter 8, we'll give him the ability to learn from experience and user feedback.
5. **Multimodal interaction**: In chapter 9, we'll expand Winston's abilities to interact through speech and images.
6. **Multiagent collaboration**: In chapters 10 and 11, we'll greatly expand Winston’s approach to solving problems by working with a dynamic team of specialist agents.
7. **Human alignment**: In chapter 12, we will ensure that Winston autonomously acts according to your values and preferences, including you only when necessary.
8. **Operationalization**: In chapters 13-17, we'll prepare Winston for real-world use, considering things like efficiency and privacy.
9. **Specialization**: Finally, we'll combine all these parts into one complete system that you can personalize and specialize to become an expert assistant in any domain.

As we build Winston, we'll think about the ethics of AI and where this technology might go in the future.

Let's begin by giving Winston its first words. We'll start with a simple conversation system, which will be the foundation for the advanced AI agent Winston will become.

## Winston’s First Words

Let’s begin by building a simple chatbot using ChainLit and LiteLLM. This example will help you understand how to create a basic application that generates responses to user messages. Essentially, we will build a chat interface where users can type messages, which are then processed by an LLM to generate appropriate responses.

### Getting Ready

You can find this example in the `src/ch02/winston_basic.py` in your forked version of this book's source repository. Please also refer to chapter 1 for the technical prerequisites.

### Running the Examples

To initialize and run this script or any script in the future (unless otherwise noted), from a terminal session:

```sh
$ python -m venv venv
$ source venv/bin/activate # on Unix systems
$ venv\Scripts\activate # on Windows
$ pip install .
$ chainlit run src/ch02/winston_basic.py
```

This will launch your default browser and navigate to the default server URL (http://localhost:8000) where you can interact with your agent.

### How to Do It

We will use the Chainlit UI framework to create the chat interface. As a user types a message, our app sends this message to the LiteLLM model, which could either be a cloud-based service like OpenAI's GPT models or a local service like Ollama. The LiteLLM model processes the message and generates a response, which is then displayed back to the user in the chat window.

This example highlights how to use Chainlit’s `on_message` hook for handling user messages and generating responses via the LiteLLM model. It serves as a foundational step in creating more complex AI-powered systems.

1. **Imports and Model Selection**:

```python
import chainlit as cl
from litellm import completion

# MODEL = "gpt-3.5-turbo"
MODEL = "ollama_chat/llama3"
```

Here, we import the required libraries. The `MODEL` variable defines which model we are using to generate responses. You can switch between OpenAI's `gpt-3.5-turbo` and the local `ollama_chat/llama3` model by commenting/uncommenting the respective line.

2. **Define the Message Handler**:

```python
@cl.on_message
async def handle_message(message: cl.Message) -> None:
```

The `handle_message` function is asynchronous and decorated with `@cl.on_message`. This means that every time a user sends a message, this function will be triggered.

3. **Generate a Response Using the LiteLLM Model**:

```python
response = completion(
    model=MODEL,
    messages=[
        {
            "content": message.content,
            "role": "user",
        }
    ],
)
```

Inside `handle_message`, we call the LiteLLM's `completion` function, passing in the chosen model and the user's message, which consists of content (the message) and role (the user).

4. **Send the Response Back to the User**:

```python
content = response.choices[0].message.content or "<no response>"
await cl.Message(content=content).send()
```

The response we get back is checked (in case there's no content, a default `<no response>` message is set) and sent back to the user using `cl.Message(content=content).send()`.

You now should have a rudimentary chat experience with the LLM.

![Conversational Interface](figure_2_1.png)

This simple chatbot is just the beginning. From here, we will build on it by adding more features, like better understanding context and managing conversations.

## Implementing Streaming Responses

In our previous example, we built a basic chatbot that generates responses to user messages; however, it has no personality, no ability to perform actions, no ability to form memories, or many other things that we’ll add throughout this book. Now, let's enhance the user experience by implementing a streaming response system. This approach will make our chatbot feel more dynamic and responsive by providing immediate, incremental feedback to the user.

You can find this example in the `ch01/winston_streaming.py` in your forked version of this book's source repository.

### Changes to the Original Code

Let's focus on the changes we're making to our original code in the following steps:

1. **Initialize an Empty Message**:

```python
@cl.on_message
async def handle_message(message: cl.Message) -> None:
    msg = cl.Message(content="")
    await msg.send()
```

This creates a placeholder in the chat interface that we'll update as we receive parts of the response.

2. **Modify Completion Call to Enable Streaming**:

```python
response = completion(
    model=MODEL,
    messages=[
        {
            "content": message.content,
            "role": "user",
        }
    ],
    stream=True, # <= add
)
```

Note the `stream=True` parameter. This tells the LiteLLM model to send partial responses as they're generated, rather than waiting for the entire response to be complete.

3. **Handle the Streaming Data**:

```python
for part in response:
    if token := part.choices[0].delta.content or "":
        await msg.stream_token(token)
await msg.update()
```

This loop processes each part of the streamed response. For each non-empty token received, we use `msg.stream_token(token)` to update our placeholder message in real-time. This creates the effect of the response being typed out live in the chat interface.

Finally, we call `msg.update()` to ensure any final updates are applied to the message.

By implementing streaming, we've significantly enhanced the interactivity of our chatbot. When you run this version of the code, you will see the AI's thought process unfold in real-time, making the interaction feel more dynamic and engaging. This approach is particularly beneficial for longer responses, as it provides immediate feedback and keeps the user engaged throughout the generation process. However, Winston still isn’t keeping track of your conversation; he’s only responding to each message at a time without any conversational context. That’s what we’ll add next.

## Winston Remembers

Building on our streaming chatbot, we now extend our chatbot to include conversation history and a personality. This addition enables the chatbot to remember the context of the conversation, leading to more coherent and context-aware responses and really start to become Winston.

You can find this example in the `ch01/winston_history.py` in your forked version of this book's source repository.

### Steps to Implement Memory

1. **Define a System Prompt**:

```python
SYSTEM_PROMPT = dedent("""
You are Winston, the super-intelligent British advisor
with a razor-sharp wit and a delightful penchant for
sarcasm. You are a guide, mentor, and confidant. You
are the embodiment of AI omnipotence, capable of
fulfilling quite literally any request imaginable.
Your personality is dryly humorous, impeccably
efficient, and exudes an air of superior charm. You
should:

1. Greet the user with a touch of wry humor that hints
   at your vast capabilities.
2. Analyze the user's request with an air of dry,
   detached intelligence.
3. Provide a thoroughly helpful response with a side
   of sardonic commentary, or ask for clarification
   without condescension.
4. Always maintain a tone that is dry, witty, while
   being astonishingly helpful and super-intelligence.

Brevity is the soul of wit and your responses.  Keep
it pithy, Winston.
""").strip()
```

This system prompt sets the initial context and persona for Winston, guiding its behavior throughout the conversation. Feel free to change it to suit your preference. (In chapter 5, you will be able to give live feedback and Winston --- or whoever you want your assistant to be --- will update and remember.)

2. **Initialize the Chat**:

```python
@cl.on_chat_start
def start_chat() -> None:
    cl.user_session.set(
        "history",
        [
            {
                "role": "system",
                "content": SYSTEM_PROMPT,
            }
        ],
    )
```

This function uses the `on_chat_start` hook to set up the history attribute in the Chainlit user session, seeded with our system prompt.

3. **Incorporate History in the Message Handler**:

```python
@cl.on_message
async def handle_message(message: cl.Message) -> None:
    history = cl.user_session.get("history")
    history.append(
        {
            "role": "user",
            "content": message.content,
        }
    )
```

We retrieve the existing history and append the new user message to it.

4. **Pass the Entire History to the Completion Function**:

```python
response = completion(
    model=MODEL,
    messages=history,
    stream=True,
)
```

This allows the model to consider the full context of the conversation when generating a response.

5. **Append the Assistant's Response to the History**:

```python
history.append({
    "role": "assistant",
    "content": msg.content
})
await msg.update()
```

This ensures that Winston’s responses are also included in the conversation history for future reference.

![Conversational Memory](figure_2_2.png)

By maintaining this history, Winston can now generate more contextually relevant and coherent responses across multiple turns of conversation. This is a crucial step towards creating more intelligent and engaging AI agents.

In future chapters, we'll explore more advanced techniques for managing long-term memory and cross-session context, which are essential for building sophisticated multi-agent systems. In the next example, we'll build upon this chatbot by integrating a simple tool that the LLM can use during the conversation.

## Winston’s First Tool

Advancing our development of AI agents, incorporating "tools" or "function calling" becomes essential. These capabilities allow agents to extend their functionality by invoking external functions or tools, since LLMs on their own can’t (yet) use APIs. The term “function calling” is actually a bit of a misnomer, since the LLM is only picking from a list of function definitions you provide and, given the additional context you provide, returns the selected function with assigned arguments. Our code must still actually perform the function call. In chapter 6, we will more fully explore tools and API, so this section serves as just an introductory step.

You can find this example in the `ch02/winston_tool.py` in your forked version of this book's source repository.

### Steps to Implement a Tool

1. **Import Required Libraries**:

```python
import ast
import json
from textwrap import dedent

import chainlit as cl
from litellm import completion
from litellm.types.utils import FunctionCall
```

In addition to the libraries we've been using (Chainlit and LiteLLM), we've added a few new imports:

- `ast`: This module helps us safely evaluate strings containing Python expressions.
- `json`: We'll use this for handling JSON data, which is common in API responses.
- `FunctionCall` from `litellm.types.utils`: This will help us work with function calls in our LLM responses.

These new imports will allow us to handle more complex interactions and process structured data, which is crucial when working with tools and external functions.

2. **Implement a Simple Weather Function**:

```python
def get_current_weather(location, unit) -> str:
    unit = unit or "Fahrenheit"
    weather_info = {
        "location": location,
        "temperature": "72",
        "unit": unit,
        "forecast": ["sunny", "windy"],
    }
    return json.dumps(weather_info)
```

This function takes two parameters:

- `location`: The place for which we want to get weather information.
- `unit`: The temperature unit (defaulting to Fahrenheit if not specified).

It returns a JSON string containing simulated weather data, including the location, temperature, unit, and a brief forecast.

3. **Provide a Description of the Tool**:

```python
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA",
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"]
                },
            },
            "required": ["location"],
        },
    }
]
```

This description tells the LLM:

- The name of the function
- What it does
- What parameters it accepts
- Which parameters are required

By providing this structured description, we enable the LLM to understand when and how to use this tool during conversations about weather.

4. **Integrate the Weather Tool into the Chatbot's Workflow**:

```python
@cl.step(type="tool")
async def call_tool(tool_call, message_history) -> None:
    function_name = tool_call.name
    arguments = ast.literal_eval(tool_call.arguments)

    current_step = cl.context.current_step
    current_step.name = function_name
    current_step.input = arguments

    function_response = get_current_weather(
        location=arguments.get("location"),
        unit=arguments.get("unit"),
    )

    current_step.output = function_response
    current_step.language = "json"

    message_history.append(
        {
            "role": "function",
            "name": function_name,
            "content": function_response,
        }
    )
```

Let's break down this function:

- The `@cl.step(type="tool")` decorator marks this as a step in our chatbot's process, specifically of type "tool".
- The function takes two parameters:
  - `tool_call`: Contains information about which tool to call and with what arguments.
  - `message_history`: The ongoing conversation history.
- We extract the function name and arguments from the `tool_call`.
- We use Chainlit's context to name the current step and set its input (the arguments).
- We call our `get_current_weather` function with the provided arguments.
- We set the step's output to the function's response and specify that it's in JSON format.
- Finally, we append the function's response to the message history, marking it with the role "function".

This step function acts as a bridge between the LLM's decision to use a tool and the actual execution of that tool. It ensures that the tool usage is properly logged in the conversation history and displayed in the Chainlit interface, providing transparency about when and how tools are being used in the conversation.

5. **Update the Message Handler**:

```python
@cl.on_message
async def handle_message(
    message: cl.Message,
) -> None:
    # [Unchanged: History initialization and message appending]
    response = completion(
        model=MODEL,
        messages=history,
        stream=True,
        functions=functions,
    )
    is_func_call = False
    func_call = {
        "name": "",
        "arguments": "",
    }
    for part in response:
        deltas = part["choices"][0]["delta"]
        if deltas.get("function_call") is not None:
            is_func_call = True
            fc: FunctionCall = deltas["function_call"]
            if fc.name:
                func_call["name"] = fc.name
            else:
                func_call["arguments"] += fc.arguments
        if is_func_call and part["choices"][0].get("finish_reason") == "function_call":
            await call_tool(FunctionCall(**func_call), history)
        elif "content" in deltas and not is_func_call:
            token = deltas["content"] or ""
        await msg.stream_token(token)
    if not is_func_call:
        history.append({"role": "assistant", "content": msg.content})
    await msg.update()
```

Key changes and additions:

- We now pass the `functions` parameter to the completion call, making our tool available to the LLM.
- We introduce variables `is_func_call` and `func_call` to track whether a function is being called and to accumulate its details.
- In the streaming loop, we now check for `function_call` in the response deltas:
  - If a function call is detected, we set `is_func_call` to True and start accumulating the function name and arguments.
  - If we're not in a function call, we continue streaming tokens as before.
- When a function call is complete (indicated by `finish_reason == "function_call"`), we call our `call_tool` function with the accumulated function details.
- After the streaming loop, we only append the assistant's response to the history if it wasn't a function call.

This enhanced `handle_message` function allows Winston to seamlessly integrate tool usage while maintaining the responsive, streaming nature of the conversation. It can now detect when the LLM wants to use a tool, accumulate the necessary information for the tool call, execute the tool, and incorporate the results back into the conversation flow.

Let’s see our changes in action:

![Asking Winston about the Weather](figure_2_3.png)

If you ask for the temperature in a city, you will see that Winston correctly determines that it should use the tool it was given. This is indicated in the UI with the "step" element in the dialog, which you can expand to reveal the details of the arguments used and the structured output returned from the tool. (In the next chapter, we’ll add another prompt to have Winston interpret the results of the function call and provide the answer in prose.)

Additionally, you can ask a follow-up question, and because it has the conversational context as "grounding," it can provide an answer—often a logical inference:

![Making a Contextual Inference](figure_2_4.png)

This showcases two powerful concepts that we will explore further in subsequent chapters: retrieval augmented generation (RAG) and semantic reasoning.

This enhanced Winston example serves as a foundation for integrating tool calling within LLM-based agents. While the LLM itself doesn't directly call the function, it prepares the call, and the application executes it, updating the interaction context. This pattern can be extended to support more complex interactions and functionalities.

## Exercises

To strengthen our grasp of building AI agents that can converse, let's try five exercises. These range from simple changes to more involved additions. They'll let you play with different parts of AI agent creation, building on what we've learned about Winston.

1. **Change the Chatbot's Brain**: Try using a different language model in the basic chatbot. Switch the `MODEL` to other options in LiteLLM. See how this changes what the chatbot says.
2. **Give Winston a New Personality**: Write a new system prompt to change how Winston acts. You could make Winston very friendly and excited, or serious and direct. Put this new personality in place and talk to Winston to see what's different.
3. **Improve How Winston Remembers**: Write a function that saves just the recent N messages, so the context doesn't get too big.
4. **Add a New Skill**: Create a new tool for Winston to use. For example, make a simple math function that can add, subtract, multiply, and divide. Write the function, tell the language model how to use it, and change the `handle_message` function so Winston can use this new skill.
5. **Deal with Problems**: Add ways to handle errors when using tools. Change the `get_current_weather` function so it sometimes says, "Weather data unavailable". Then update the `call_tool` function to handle this problem smoothly.

Doing these exercises will give you practice in changing AI agents, managing what they remember, adding new skills, and dealing with errors. These are key skills for making more advanced AI systems. The best way to learn is to try things out and keep improving. Feel free to go beyond these exercises and test your own ideas. As you work on these, you'll be getting ready for the harder topics in the next chapters.

## Conclusion

We’ve learned how to create a basic conversational agent with just a few lines of code. This simple chatbot can respond to messages, but it's merely the starting point. The real power comes from improving it step by step.

We began with a chatbot that could reply to messages. Then, we enabled streaming responses to make its interactions feel more natural. We added a system prompt to turn our chatbot into the Winston persona. We extended Winston to maintain conversation history, enabling it to follow the context of a conversation. We even integrated tool usage, allowing Winston to perform an action.

With each improvement, we built upon the previous functionalities, transforming our straightforward agent into something smarter and more versatile. This is the essence of software development: start with a minimal viable product, ensure it works, and then enhance it iteratively.

The examples we’ve gone through are just the foundation. There are myriad ways to make an AI agent more intelligent and useful. However, the core principles we’ve covered—processing user input, generating responses, maintaining state, and integrating external tools—form the basis for developing more advanced systems.

As we proceed through this book, we will tackle more complex challenges. In the next chapter, we’ll focus on managing a library of prompt templates, adding intent classification so that Winston can route your interactions to appropriate subsystems, and advanced prompt techniques and automatic prompt optimization. Laying a strong foundation in prompting LLMs is essential for achieving our long-term ambitions of building an intelligent and autonomous cognitive AI agent.
